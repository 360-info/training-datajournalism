{
  "hash": "f4c20587d5912f7a8c18d8446f9e9b87",
  "result": {
    "markdown": "---\ntitle: \"Scraping cards and dashboards\"\nresources: suburbs.yml\nlisting:\n  id: suburbs\n  contents: [\"suburbs.yml\"]\n  sort: [\"name\"]\n  filter-ui: true\n  type: grid\n  page-size: 10000\n---\n\n\nFor this demo, we've set up what looks like a state government dashboard, with some statistics for ~~each suburb~~ the 100 most populous suburbs in Melbourne pulled from the [Australian Bureau of Statistics](https://www.abs.gov.au). Each card has a number of relevant pieces of information.\n\nHow would we go about extracting the info from these cards into Google Sheets? If it was a `<table>`, we could just use the `importhtml()` function to get the whole thing in one go.\n\nWe can't do that this timeâ€”it isn't a table.\n\n:::{.callout-note collapse=\"true\"}\n## Recreating this exercise\n\nI've cheated and created this listing from a spreadsheet myselfâ€”but it mimics the sorts of listings you'll see on product listings and dashboards all over the internet. If you're interested, the code here shows I collected and transformed ABS spreadsheets to make this exercise.\n\nStep 1: Downloading Census DataPacks\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(scales)\nlibrary(yaml)\nlibrary(here)\n\nzip_path <- here(\"lessons\", \"scraping-cards\", \"vic-stats.zip\")\nzip_url <- paste0(\n  \"https://www.abs.gov.au/\",\n  \"census/find-census-data/datapacks/download/\",\n  \"2021_GCP_SAL_for_VIC_short-header.zip\")\ndownload.file(zip_url, zip_path)\nunzip(zip_path, exdir = here(\"lessons\", \"scraping-cards\", \"data\"))\nfile.rename(\n  here(\"lessons\", \"scraping-cards\", \"data\", \"2021 Census GCP Suburbs and Localities for VIC\"),\n  here(\"lessons\", \"scraping-cards\", \"data\", \"Responses\"))\n```\n:::\n\n\nStep 2: Loading Census tables and tidy them up\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# suburb codes and names\nhere(\"lessons\", \"scraping-cards\", \"data\", \"Metadata\",\n  \"2021Census_geog_desc_1st_2nd_3rd_release.xlsx\") |>\n  read_excel(sheet = \"2021_ASGS_Non_ABS_Structures\") |>\n  filter(ASGS_Structure == \"SAL\") |>\n  select(Code = Census_Code_2021, Name = Census_Name_2021) ->\nsuburb_map\n\n# total population\nhere(\"lessons\", \"scraping-cards\", \"data\", \"Responses\", \"2021Census_G01_VIC_SAL.csv\") |>\n  read_csv(col_select = c(SAL_CODE_2021, Tot_P_P)) |>\n  mutate(`Total population` = as.integer(Tot_P_P)) |>\n  select(SAL_CODE_2021, `Total population`) ->\npopulation\n\n# income and rent (format as currencies)\nhere(\"lessons\", \"scraping-cards\", \"data\", \"Responses\", \"2021Census_G02_VIC_SAL.csv\") |>\n  read_csv(col_select =\n    c(SAL_CODE_2021, Median_rent_weekly, Median_tot_fam_inc_weekly)) |>\n  mutate(\n    Median_rent_weekly = label_dollar(accuracy = 1)(Median_rent_weekly),\n    Median_tot_fam_inc_weekly =\n      label_dollar(accuracy = 1)(Median_tot_fam_inc_weekly)) |>\n  rename(\n    `Median weekly rent` = Median_rent_weekly,\n    `Median weekly family income` = Median_tot_fam_inc_weekly) ->\nincome_and_rent\n\n# commuting:\n# we just want the most popular commute method for each area,\n# which i'll encode as emoji\nhere(\"lessons\", \"scraping-cards\", \"data\", \"Responses\", \"2021Census_G62_VIC_SAL.csv\") |>\n  read_csv(\n    col_types = cols(SAL_CODE_2021 = col_character(), .default = col_integer()),\n    col_select = c(SAL_CODE_2021, ends_with(\"_P\"))) |>\n  select(SAL_CODE_2021, matches(\"One_method\"), matches(\"Two_methods\"),\n    matches(\"Three_meth\"), -matches(\"Tot\")) |>\n  pivot_longer(-SAL_CODE_2021, names_to = \"method\", values_to = \"count\") |>\n  filter(count > 0) |>\n  group_by(SAL_CODE_2021) |>\n  slice_max(count, n = 1) |>\n  ungroup() |>\n  mutate(\n    \"Most popular commute method\" = str_replace_all(method, c(\n      \"Train\" = \"ðŸš‚ Train\",\n      \"Trn\" = \"ðŸš‚ Train\",\n      \"Bus\" = \"ðŸšŒ Bus\",\n      \"Ferry\" = \"â›´ï¸ Ferry\",\n      \"Car_as_driver\" = \"ðŸš— Driving\",\n      \"Car_as_drvr\" = \"ðŸš— Driving\",\n      \"Car_as_passenger\" = \"ðŸš— Passenger\",\n      \"Car_as_pass\" = \"ðŸš— Passenger\",\n      \"Truck\" = \"ðŸšš Truck\",\n      \"Motorbike_scootr\" = \"ðŸ›µ Motorbike or scooter\",\n      \"Other\" = \"â“ Other\",\n      \"Walked_only\" = \"ðŸš¶ Walk\",\n      \"_P\" = \"\",\n      \"Tr_2_oth_meth\" = \"ðŸš‚ Train and two other methods\",\n      \"Othr_three_meth\" = \"â“ Three other methods\",\n      \"One_method\" = \"\",\n      \"Two_methods\" = \"\",\n      \"Three_meth\" = \"\",\n      \"_\" = \" \"))) |>\n  select(-method, -count) ->\ncommuting_mostpopular\n```\n:::\n\n\nStep 3: Joining the tidied tables and outputting them as YAML (in order to make the listing below)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# join and write out biggest 100 suburbs to yaml (so we can make a listing)\npopulation |>\n  left_join(income_and_rent, join_by(SAL_CODE_2021)) |>\n  left_join(commuting_mostpopular, join_by(SAL_CODE_2021)) |>\n  left_join(suburb_map, join_by(SAL_CODE_2021 == Code)) |>\n  select(Name, Code = SAL_CODE_2021, everything()) |>\n  mutate(`Total population` = as.integer(`Total population`)) |>\n  rename(title = Name) |>\n  replace_na(list(`Most popular commute method` = \"\")) |>\n  slice_max(`Total population`, n = 100) ->\njoined\n\nwrite_yaml(joined, here(\"lessons\", \"scraping-cards\", \"suburbs.yml\"), column.major = FALSE)\n```\n:::\n\n:::\n\n::::{.callout-tip collapse=\"true\"}\n## Solution\n\nWe can use Google Sheets' more general `importxml()` function to get information that is in all sorts of structuresâ€”not just tables!\n\nThe `importxml()` function takes a page address too, but the second thing we have to tell it is called an `XPath`. An `XPath` is a kind of address for looking up content on a web page.\n\nFor example, to extract the title from each of the cards below, we would write the following:\n\n```\n=importxml(\n  \"https://360-info.github.io/training-datajournalism/lessons/scraping-cards\",\n  \"//h5\")\n\n```\n\nThe first part is the URL of this page; the second tells the scraper to look for fifth-level headings (`h5`). In other words, the heading from each card. (We'd want to make sure there weren't any other fifth-level headings on the page, or else we'd want to be more specific!)\n\nTo get the income, we would use:\n\n```\n=importxml(\n  \"https://360-info.github.io/training-datajournalism/lessons/scraping-cards\",\n  \"//td[@class=\"Median weekly rent\"]\")\n```\n\nThis is pretty similar, but instead of looking for headings inside cards, we're looking for table cells (`td`) that have the class `Median weekly rent`. That's because each card on this page has a little table inside it.\n\n:::{.callout-note collapse=\"true\"}\n## Wait, what's a `class`?\n\nElements on web pages can have a unique `id`, as well as one or more `class`es to help describe them. The cards on this page have the class `card`, and the pieces of information on each card have a class named for the data (its 'column', if this were a spreadsheet).\n\nEvery web page is arranged differently!\n:::\n\nLearning how to write XPath can take time, and it involves learning about web pages are strcutured. But this demo shows you the power you have with common tools to extract dataâ€“even from places where the authors haven't made it easy to access!\n\nIf you'd like to learn more, here are some resources:\n\n- [Mozilla Development Network: Getting Started with HTML](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML/Getting_started)\n- [W3C: XPath](https://www.w3schools.com/xml/xpath_intro.asp)\n\n::::\n\n::::{.column-page}\n:::{#suburbs}\n:::\n::::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}